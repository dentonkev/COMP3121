# 3121 Notes

## General Info

**Lecturer**: Aleksandar Ignjatovic  
**Course email**: cs3121@cse.unsw.edu.au  
**Lab tutorial times**: https://timetable.unsw.edu.au/2024/COMP3121.html#S2S

https://www.bigocheatsheet.com/  
https://haezera.github.io/#/resources

### Modules
1. Foundations (week 1)
2. Divide and conquer (week 2 - 3)
3. Greedy algorithms (weeks 4 – 5)
4. Flow networks (week 5 - 7)
5. Dynamic programming (weeks 7 – 9)
6. Intractable problems (weeks 9 – 10)

## Algorithms
An algorithm is a **sequence of precisely defined steps** that can be executed mechanically.

**Sequential Algorithms**
- Comprised by a sequence of steps, which can only be **executed one at a time.**

**Deterministic Algorithms**
- Each step always gives the same result for the same input

## Efficiency 
Measure the efficiency of algorithms using **asymptotic analysis**.
- we are concerned with the **worst case** performance, however average performance sometimes accepted (quicksort)

### Big-Oh
$f(n) = O(g(n))$ if for large enough $n$, $f(n)$ is at most a constant multiple of $g(n)$.

- $g(n)$ is said to be an asymptotic upper bound for $f(n)$

<img src="./images/big.png" width='500px' height='auto'>

### Big-Omega
$f(n) = Ω(g(n))$ if for large enough $n$, $f(n)$ is at least a constant multiple of $g(n)$.

- $g(n)$ is said to be an asymptotic lower bound for $f(n)$

### Big-Theta
$f(n) = Θ(g(n))$ if $f(n)$ is equal to $g(n)$.
- Only true if $f(n) = O(g(n))$ and $f(n) = Ω(g(n))$.

### Summary
<img src="./images/time.png" width='600px' height='auto'>

### Sum property 
If $f_1 = O(g_1)$ and $f_2 = O(g_2)$, then $f_1 + f_2 = O(g_1+g_2)$

### Product property
If $f_1 = O(g_1)$ and $f_2 = O(g_2)$ ,then $f_1 \cdot f_2 = O(g_1 \cdot g_2)$

## (Binary) Heaps
- Max heap, parent $\geq$ children
- Min heap, parent $\leq$ children
- Used to implement priority queue

#### Performance
- Build heap: $O(n)$
- Find maximum: $O(1)$
- Delete maximum: $O(log n)$
- Insert: $O(log n)$

<img src="./images/heap2.png" width='500px' height='auto'>

Parent at index `i`, children index `2i, 2i + 1`

## Binary Search Tree
Each node’s value $\geq$ all values in left subtree, and $\leq$ all keys in right subtree.

#### Performance
Let h be the height of tree
- Build heap: $O(h)$
- Insert/delete: $O(h)$

<img src="./images/tree2.png" width='500px' height='auto'>

### Self Balancing BST
- Perform rotations to ensure $h = O(log(n))$
- All operations:  $O(log(n))$

## Hash Tables
- Store **values** indexed by **keys**.
- Hash function maps keys to indices in a fixed size table.
- Ideally no two keys map to the same index.

#### Performance (expected)
- Search for value associated to key: $O(1)$
- Update value associated to key: $O(1)$
- Insert/delete: $O(1)$
  
#### Performance (worst case)
- Search for value associated to key: $O(n)$
- Update value associated to key: $O(n)$
- Insert/delete: $O(n)$

## Reasoning about algorithms
Whenever we present an algorithm, we must justify its **correctness** and **efficiency**.

## Gale-Shapley algorithm
**1-foundation-short.pdf**:  slides 40 - 55

- The algorithm terminates after $\leq n_2$ rounds.
- The algorithm produces a perfect matching, everyone belongs to exactly one pair.
- The matching produced by the algorithm is stable.

## Divide and Conquer
- This method is called **divide-and-conquer** to develop algorithms
- Examples: binary search, merge sort and quicksort.

## Binary Search
- **Divide**: Test the midpoint of the search range $Θ(1)$  
- **Conquer**: Search one side of the midpoint recursively  
- **Combine**: Pass the answer up the recursion tree $(Θ(1))$
- **Recursion** is $log_2n$ levels deep, with a total of $Θ(1)$ time spent in each level.

Time complexity is $Θ(log n)$.

## Merge sort
- **Divide**: Split the array into two equal part $Θ(1)$  
- **Conquer**: Sort each part recursively
- **Combine**: Merge the two sorted subarrays (Θ(n))
- **Recursion** is $log_2n$ levels deep, with a total of $Θ(n)$ time spent in each level.

Time complexity is $Θ(n log n)$.

### Inversions
An inversion is a pair $(i,j)$ such that:
- $i$ < $j$ and $A[i] > B[j]$

## Quick Sort
- **Divide**: Choose a pivot and partition the array around it $(Θ(n))$
- **Conquer**: Sort both sides of the pivot recursively
- **Combine**: Pass the answer up the recursion tree $(Θ(1))$
- **Recursion** Typically $log_2n$ levels deep, with a total of $Θ(n)$ time spent in each level.

$Θ(n log n)$ in average case  
$Θ(n^2)$ worst case

## Recurrences
Recurrences are important to us because they arise in estimations of time complexity of divide-and-conquer algorithms.
- reduces a problem of size $n$ to $a$ many problems each of smaller size $\frac{n}{b}$
- overhead cost $f(n)$ to split and combine smaller problems

Time complexity: $T(n) = aT(\frac{n}{b}) + f(n)$  
- Merge sort: **a = 2** (since you have to recurse on two half), **b = 2** (since splitting array in half)
- Binary search: **a = 1** (since recurse on only one half), **b = 2** (since splitting array in half)

<img src="./images/rec.png" width='500px' height='auto'>  

- depth of tree: instance in level $k$ have size $n/b^k$
- number of leaves: $a^k$ instances at depth $k$, so num leaves = a^$(log_bn)$ = n^($log_ba$)
- time spent at leaves: leaf are base case so $Θ(n^(log_ba))$

## Master Theorem
<img src="./images/m1.png" width='500px' height='auto'>  
<img src="./images/m2.png" width='500px' height='auto'>  
<img src="./images/m3.png" width='500px' height='auto'>  
