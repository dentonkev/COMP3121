# 3121 Notes

## General Info

**Lecturer**: Aleksandar Ignjatovic  
**Course email**: cs3121@cse.unsw.edu.au  
**Lab tutorial times**: https://timetable.unsw.edu.au/2024/COMP3121.html#S2S

https://www.bigocheatsheet.com/  
https://haezera.github.io/#/resources

### Modules
1. Foundations (week 1)
2. Divide and conquer (week 2 - 3)
3. Greedy algorithms (weeks 4 – 5)
4. Flow networks (week 5 - 7)
5. Dynamic programming (weeks 7 – 9)
6. Intractable problems (weeks 9 – 10)

## Algorithms
An algorithm is a **sequence of precisely defined steps** that can be executed mechanically.

**Sequential Algorithms**
- Comprised by a sequence of steps, which can only be **executed one at a time.**

**Deterministic Algorithms**
- Each step always gives the same result for the same input

## Efficiency 
Measure the efficiency of algorithms using **asymptotic analysis**.
- we are concerned with the **worst case** performance, however average performance sometimes accepted (quicksort)

### Big-Oh
$f(n) = O(g(n))$ if for large enough $n$, $f(n)$ is at most a constant multiple of $g(n)$.

- $g(n)$ is said to be an asymptotic upper bound for $f(n)$

<img src="./images/big.png" width='500px' height='auto'>

### Big-Omega
$f(n) = Ω(g(n))$ if for large enough $n$, $f(n)$ is at least a constant multiple of $g(n)$.

- $g(n)$ is said to be an asymptotic lower bound for $f(n)$

### Big-Theta
$f(n) = Θ(g(n))$ if $f(n)$ is equal to $g(n)$.
- Only true if $f(n) = O(g(n))$ and $f(n) = Ω(g(n))$.

### Summary
<img src="./images/time.png" width='600px' height='auto'>

### Sum property 
If $f_1 = O(g_1)$ and $f_2 = O(g_2)$, then $f_1 + f_2 = O(g_1+g_2)$

### Product property
If $f_1 = O(g_1)$ and $f_2 = O(g_2)$ ,then $f_1 \cdot f_2 = O(g_1 \cdot g_2)$

## (Binary) Heaps
- Max heap, parent $\geq$ children
- Min heap, parent $\leq$ children
- Used to implement priority queue

#### Performance
- Build heap: $O(n)$
- Find maximum: $O(1)$
- Delete maximum: $O(log n)$
- Insert: $O(log n)$

<img src="./images/heap2.png" width='500px' height='auto'>

Parent at index `i`, children index `2i, 2i + 1`

## Binary Search Tree
Each node’s value $\geq$ all values in left subtree, and $\leq$ all keys in right subtree.

#### Performance
Let h be the height of tree
- Build heap: $O(h)$
- Insert/delete: $O(h)$

<img src="./images/tree2.png" width='500px' height='auto'>

### Self Balancing BST
- Perform rotations to ensure $h = O(log(n))$
- All operations:  $O(log(n))$

## Hash Tables
- Store **values** indexed by **keys**.
- Hash function maps keys to indices in a fixed size table.
- Ideally no two keys map to the same index.

#### Performance (expected)
- Search for value associated to key: $O(1)$
- Update value associated to key: $O(1)$
- Insert/delete: $O(1)$
  
#### Performance (worst case)
- Search for value associated to key: $O(n)$
- Update value associated to key: $O(n)$
- Insert/delete: $O(n)$

## Reasoning about algorithms
Whenever we present an algorithm, we must justify its **correctness** and **efficiency**.

## Gale-Shapley algorithm
**1-foundation-short.pdf**:  slides 40 - 55

- The algorithm terminates after $\leq n_2$ rounds.
- The algorithm produces a perfect matching, everyone belongs to exactly one pair.
- The matching produced by the algorithm is stable.

# Divide and Conquer
- This method is called **divide-and-conquer** to develop algorithms
- Examples: binary search, merge sort and quicksort.

## Binary Search
- **Divide**: Test the midpoint of the search range $Θ(1)$  
- **Conquer**: Search one side of the midpoint recursively  
- **Combine**: Pass the answer up the recursion tree $(Θ(1))$
- **Recursion** is $log_2n$ levels deep, with a total of $Θ(1)$ time spent in each level.

Time complexity is $Θ(log n)$.

## Merge sort
- **Divide**: Split the array into two equal part $Θ(1)$  
- **Conquer**: Sort each part recursively
- **Combine**: Merge the two sorted subarrays (Θ(n))
- **Recursion** is $log_2n$ levels deep, with a total of $Θ(n)$ time spent in each level.

Time complexity is $Θ(n log n)$.

### Inversions
An inversion is a pair $(i,j)$ such that:
- $i$ < $j$ and $A[i] > B[j]$

## Quick Sort
- **Divide**: Choose a pivot and partition the array around it $(Θ(n))$
- **Conquer**: Sort both sides of the pivot recursively
- **Combine**: Pass the answer up the recursion tree $(Θ(1))$
- **Recursion** Typically $log_2n$ levels deep, with a total of $Θ(n)$ time spent in each level.

$Θ(n log n)$ in average case  
$Θ(n^2)$ worst case

## Recurrences
Recurrences are important to us because they arise in estimations of time complexity of divide-and-conquer algorithms.
- reduces a problem of size $n$ to $a$ many problems each of smaller size $\frac{n}{b}$
- overhead cost $f(n)$ to split and combine smaller problems

Time complexity: $T(n) = aT(\frac{n}{b}) + f(n)$  
- Merge sort: **a = 2** (since you have to recurse on two half), **b = 2** (since splitting array in half)
- Binary search: **a = 1** (since recurse on only one half), **b = 2** (since splitting array in half)

<img src="./images/rec.png" width='500px' height='auto'>  

- depth of tree: instance in level $k$ have size $n/b^k$
- number of leaves: $a^k$ instances at depth $k$, so num leaves = a^$(log_bn)$ = n^($log_ba$)
- time spent at leaves: leaf are base case so $Θ(n^(log_ba))$

## Master Theorem
<img src="./images/m1.png" width='450px' height='auto'>  
<img src="./images/m2.png" width='450px' height='auto'>  
<img src="./images/m3.png" width='450px' height='auto'>  

## Master Theorem Examples
### Theorem 1
<img src="./images/m4.png" width='450px' height='auto'>  

### Theorem 2
<img src="./images/m5.png" width='450px' height='auto'>  

### Theorem 3
<img src="./images/m8.png" width='450px' height='auto'>  
<img src="./images/m6.png" width='450px' height='auto'>  
<img src="./images/m7.png" width='450px' height='auto'>  

## Karatsuba Trick
- Multiplying two integer $A$ and $B$
- Split $A$ into $A_1 = \frac{n}{2}$ left bits, $A_0 = \frac{n}{2}$ right bits
- $A$ = $A_1 \times 2^{\frac{n}{2}} + A_0$
  - $\times2^{\frac{n}{2}}$, shifts the bits $\frac{n}{2}$ to left. (think $\times$ 10 in decimal)
- $B$ = $B_1 \times 2^{\frac{n}{2}} + B_0$ (same as above)
- In previous method there are 4 multiplications: $A_1B_1$, $A_1B_0$, $A_0B_1$, $A_0B_0$
- In Karatsuba trick there are 3 multiplcations (saves 1): $A_1B_1$, $(A_1+A_0)(B_1+B_0)$, $A_0B_0$
  
<img src="./images/karat.png" width='450px' height='auto'>  

### Karatsuba Trick Time complexity
<img src="./images/n.png" width='450px' height='auto'>  

## Polynomials
### Coefficient Representation
$P(x) = A_nx^n + A_{n-1}x^{n-1} + ... + A_1x+A_0$

- **Addition**: Allows for addition of two polynomials $P_A$ and $P_B$ term by term 
  -  $O(n)$ time 
- **Evaluation**: All terms have a factor of $x$ except $A_0$
  - $P(x) = A_0 + x(A_1 + A_2x + ... + A_{n-1}x^{n-2} + A_nx^{n-1})$ 
  - $P(x) = A_0 + x(A_1 + x(A_2 + x(...)))$
  - Start from middle $A_n$, then multiply by $x$, add $A{n−1}$, multiply by $x$, and so on.
  - $O(n)$ time.
- **Multiplication**: Involves multiplying every $A_iB_j$ pair
  - $(A_nx^n ... + A_1x+A_0)(B_nx^n ... + B_1x+B_0)$
  - $O(mn)$ time

### Polynomial Multiplication
$P(x) = A_nx^n + A_{n-1}x^{n-1} + ... + A_1x+A_0$   
and  
 $P(x) = B_nx^n + B_{n-1}x^{n-1} + ... + B_1x+A_0$

Product will be of degree $2n$

$P_C(x) = C_{2n}x^{2n} + C_{2n-1}x^{2n-1} + ... + C_1x+C_0$

<img src="./images/poly.png" width='450px' height='auto'>  

## Convolution
<img src="./images/conv.png" width='450px' height='auto'>  

- Using formula above, product of two polynomials can be computed in $\Theta(n^2)$ time, as there are quadractic many $A_iB_j$ products 

### Visualisation of $A \star B$
<img src="./images/c.png" width='450px' height='auto'> 


### Value representation
Represent a polynomial of degree $n$ by a **sequence** of $n + 1$ values.
- Fix a selection of inputs $x_0, . . . , x_n$. Then the corresponding values $P(x_0), . . . , P(x_n)$ represent a unique polynomial of degree up to $n$.

<br>

- **Addition and Multiplication**: can be executed **pointwise** in linear time:
  - $(P_A + P_B)(x_0) = P_A(x_0) + P_B(x_0)$
  - $(P_AP_B)(x_0) = P_A(x_0)P_B(x_0)$
- **Evaluation**: No straightforward way to evaluate input which isnt in $x_i$

### Converting between coefficient and value

<img src="./images/rep.png" width='450px' height='auto'>   

## Complex root of unity
<img src="./images/u.png" width='450px' height='auto'>  
<img src="./images/cancel.png" width='450px' height='auto'>  

Example: If $k = 3$, $m = 4$ and $l = 2$,  
$w^k_m$ = $w^3_4$ = $w^6_8$

## Discrete Fourier Transfrom
<img src="./images/dft.png" width='500px' height='auto'>  

## Fast Fourier Transform
https://www.youtube.com/watch?v=h7apO7q16V0

**High Level Explanation**  
Example: $S \times V$:
- Use $FFT(S$ and $V) => DFT(S)$ and $DFT(V)$ 
- Multiply $DFT(S)$ and $DFT(V)$ pointwise 
- Use $IFFT$ to get $S \times V$

<img src="./images/fft.png" width='500px' height='auto'>  

<br>

# Greedy Algorithms
A **greedy algorithm** is one that solves a problem by **dividing it into stages** by considering the choice that appears best at that stage.

### Examples
- **Optimal Selection** (Activity Selection, Cell Towers)
- **Optimal Ordering** (Minimising Job Lateness)
- **Optimal Merging** (Huffman Tree)
- **Direced Graph Structure** (Tsunami Warning)

## Direced Graph Structure

### Strongly connected Components
Given a directed graph $G = (V, E)$ and a vertex $v$ , the **strongly connected component** of $G$ containing $v$ consists of all vertices $u ∈ V$ such that there is a **path** in G from $v to u$ and a **path** from $u to v$. This is $C_v$.

- Use **Tarjan’s Algorithm** and **Kosaraju Algorithm** to find strongly connected components in $O(V + E)$ time. 

### Condensation Graphs
<img src="./images/cond.png" width='500px' height='auto'>

- **Verticies** are strongly connected 
- **Edges** connect different strongly connected components.
- Condensation graphs are **directed acyclic graphs**
  
<img src="./images/c1.png" width='350px' height='auto'>   
<img src="./images/c2.png" width='350px' height='auto'>   

### Topological Sorting
<img src="./images/top.png" width='450px' height='auto'>   

- $O(V + E)$, linear time for a graph

## Single Shortest Paths
**Dijkstra's algorithm's** finds the single shortest path from one node to another.

### Array Implementation
- $n$ linear scans of array of length $n$ --> $O(n^2)$
- check for $m$ edges `d_v + w(v,z) < d_z` --> $O(m)
  - Overall $O(m + n^2)$

### Augmented Heap Implementation
- Heap represented by array $A[1..n]$, where $A[j] = (i, d_i)$
  - Min-heap property on $d$ values
  - Left child at $A[2j]$
  - Right child of $A[2j + 1]$

Since heaps can only get min/max, use second array to store **position** of elements in heap.
- $A[j]$ refers to vertex $i$, so record $P[i] = j$
- So to lookup vertex $i$, $A[P[i]] = (i, d_i)$

<img src="./images/aug.png" width='500px' height='auto'>  

#### Heap operations
- Updating d-value at vertex $i$ is $O(logn)$
- Accesing top of heap is $O(1)$ and popping from heap is $O(logn)$

<img src="./images/aug2.png" width='475px' height='auto'>  

#### Time Complexity
- Each of $n$ stages requires deletion from heap --> $O(nlogn)$
- Each of $m$ edges causes at most one update in heap --> $O(mlogn)$
- Overall $O((m + n)logn)$, since $m \geq n - 1$ --> $O(mlogn)$

## Minimum spanning trees
A minimum spanning tree $T$ of a connected graph $G$ is a subgraph of $G$ (with the same set of vertices) which is a tree, and among all such trees it **minimises** the total length of all edges in T.

###  Kruskal's Algorithm
- sort the edges $E$ by weight
- add edge $e$ if no cycle, or discard otherwise
- process terminates when forest is connected

### Union-Find
**Union-Find** data structure used to implement Kruskal's algorithm
- `FIND(a)` should find the root of tree containing $a$
- `UNION(a,b)` 
  - do nothing if $a$ and $b$ in same tree 
  - add an **edge** between two trees so they become one tree

<img src="./images/uf.png" width='550px' height='auto'>  

#### `FIND(a)` steps
- starting at $a$, recursively traverse edge to parent until **root** of tree is found.
- $O(h)$ --> $O(logn)$

#### `UNION(a,b)` steps
- Run `FIND(a)` and `FIND(b)` to identity roots $c, d$
- If $c = d$ do nothing
- Otherwise, place edge between $c, d$
  - If $subtree(c) \geq subtree(d)$, then set $parent(d) = c$
  - If $subtree(d) > subtree(c)$, then set $parent(c) = d$
- $O(h)$ --> $O(logn)$  

<img src="./images/size.png" width='550px' height='auto'>  

### Size Heuristic

<img src="./images/heu.png" width='500px' height='auto'>  

### Time Complexity of Union-Find
- `FIND(a)` --> $O(logn)$
- `UNION(a,b)` --> $O(logn)$

### Applying Kruskal's using Union-Find

- Algorithm explanation

<img src="./images/uiaf.png" width='500px' height='auto'>  

- Analysing time complexity, overall --> $O(mlogn)$
  
<img src="./images/t.png" width='500px' height='auto'>  
